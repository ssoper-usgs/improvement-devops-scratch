# improvement-devops-scratch

This exercise will present you with application code and an ongoing operational issue.
You will be provided with a brief background regarding the purpose of the application,
some details regarding the operational issues, and then questions for you to consider.
You need not answer all the questions nor be constrained by them--the goal of this
exercise for you to get a sense of the domain and for us to understand how you think 
and problem-solve.

## Background

This repo contains two AWS Lambda functions intended to help with QA/QC of data coming
from a UV-Vis spectrophotometer. Data from the spectrophotometer lands in an SQS queue and
subsequently triggers the data check Lambda function. If a data issue is
identified, another Lambda is triggered to send a notification to lab staff.

## Operational Observations

Here are some observations that a developer has noticed regarding the system:

* Runtimes for both Lambdas are about 200 ms. 
* The quality check Lambda reports using 108 MB of memory on average.
* The notification Lambda reports using 90 MB on memory of average.
* The quality check Lambda works 99.9 % of the time.
* The notification Lambda fails 10 % of the time. No one is notified after the 
failure, and there's no record of the erroneous data.
* It is difficult to figure out which sample's data might have caused a failure.


## Prompts

Please submit you exercise responses to eread@usgs.gov either in the body of an email or in an attachment. Each 
question maybe addressed with a few sentences or a short paragraph.

Questions to consider:

1. What is the purpose of the serverless.yml file?
- The serverless.yml file is a config file used to define assets being deployed to aws.
2. What is the purpose of the pyproject.toml file?
- The .toml file is the config file used by the poetry dependency management tool.  Sort of like combining Python's built-in pip and virtualenv into one easy-to-use tool.
3. What other pieces of information would you want to see to help investigate the operational issues? How might that information be acquired?
- It would be good to see monitoring of all the assets in the serverless stack to get a sense of overall performance over time.  This could be a cloudwatch dashboard generated by the developer or simply navigating to the deployed assetâ€™s UI page. A log query tool would also be useful, something like cloudwatch logs insights. This information would help determine when the error started and how prevalent it is.  Once a fix is in place, this same information can be used to see if the error rate improves.
4. What are your thoughts on code quality and tests within the repo? What tools and techniques could be used to improve it?
- The existing code and tests seem fine and meaningful for the code being exercised.  However, without a coverage tool it's hard to know exactly which lines of code are covered at a glance. Locally and as part of a CI script, you could add coverage or pytest-cov to get a better picture of coverage metrics and address any weak points. I don't see a CI/CD tool as part of the codebase, but if this code were to be deployed I would want to see a build/deploy tool like travis-CI, code quality tools like codacy, and code coverage tools included so that unit tests/coverage/quality metrics were displayed on merge requests as part of a peer review process to ensure the best code gets to production.
5. Are there improvements that you would make to the application code to address the operational issues?
- I want to see an error stack trace when the failure happens, along with any other useful info about the failed record. This can be achieved by logging the exception in the except block of the `data_quality_check_handler` method in the `handler.py` module.  I would also raise an error there instead of passing so the lambda fails when the notification fails, making it more clear to a dev/ops person that something is wrong. From there we could figure out why the notification fails to send and iterate to make it more reliable. Also the `awslambda.py` module's invoke method returns the lambda invocation response, but the response is not interpreted for success or failure anywhere. It would be helpful to raise an exception if the lambda response came back as non-successful.
6. Are there improvements that you would make to the infrastructure-as-code to address the operational issues?
- I would add an sqs queue and have the `data_quality_check_handler` function populate it with events (failed qc checks). I would retool the `awslambda.py` module to instead populate an sqs queue instead of invoking a lambda. I would add an event trigger for this new queue to the existing `problemNotifier` lambda so that it triggered off these events, rather than being triggered via lambda invocation in the code via the other lambda, `data_quality_check_handler`. Coupled with better exception handling, re-organizing the code this way would make it easier to figure out which lambda is failing and why.
7. Are there optimizations within the infrastructure-as-code that could potentially reduce operational costs?
- The developer mentions each lambda uses 90 to 108mb of memory, but in the configuration these assets are each set to use 512mb.  This might add extra cost to these invocations. To reduce costs, one could lower the provisioned memory to 256 or 128mb.  If there is not a performance hit, that should save costs. However, the extra memory might make the lambdas so fast that the cost is made up in lower invocation times. Experimentation to determine the best value would be a good approach.
8. Are there different tools and/or different ways of putting things together that would be more effective?
- Addressed in 6 above, but to add a bit more: separating the lambdas by a queue would offer better scaling opportunities.  If one lambda sees more invocations than the other, for instance, it could have much higher concurrency values and its batch size could be increased to accommodate the load, without affecting the other lambda.  You could individually provision each asset to maximize the performance/cost ratio without one lambda's performance being bottlenecked by the other.
9. Are there any other improvements that you can think of?
- I would add doc in the README about how the unit tests work.

## Running the unit tests:
```shell
# only works off vpn
poetry shell
poetry install
pytest

# beyond this, I don't think we have coverage tools at the ready here, but would be good to add one.  Something like
# coverage would be fine, or since we have pytest already, pytest-cov would also work.
```
